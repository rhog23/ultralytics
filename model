digraph {
	graph [size="62.25,62.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	133067591928880 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	133067658894816 [label=AddmmBackward0]
	133067591433568 -> 133067658894816
	133067659231008 [label="model.1.linear.bias
 (1000)" fillcolor=lightblue]
	133067659231008 -> 133067591433568
	133067591433568 [label=AccumulateGrad]
	133067658895584 -> 133067658894816
	133067658895584 [label=ViewBackward0]
	133067591917168 -> 133067658895584
	133067591917168 [label=MeanBackward1]
	133067591913376 -> 133067591917168
	133067591913376 [label=SiluBackward0]
	133067591681648 -> 133067591913376
	133067591681648 [label=NativeBatchNormBackward0]
	133067592193680 -> 133067591681648
	133067592193680 [label=ConvolutionBackward0]
	133067659238352 -> 133067592193680
	133067659238352 [label=ReluBackward0]
	133067659235856 -> 133067659238352
	133067659235856 [label=AddBackward0]
	133067591910928 -> 133067659235856
	133067591910928 [label=NativeBatchNormBackward0]
	133067592194064 -> 133067591910928
	133067592194064 [label=ConvolutionBackward0]
	133067592194256 -> 133067592194064
	133067592194256 [label=ReluBackward0]
	133067592194400 -> 133067592194256
	133067592194400 [label=NativeBatchNormBackward0]
	133067592194496 -> 133067592194400
	133067592194496 [label=ConvolutionBackward0]
	133067592193872 -> 133067592194496
	133067592193872 [label=ReluBackward0]
	133067592194784 -> 133067592193872
	133067592194784 [label=AddBackward0]
	133067592194880 -> 133067592194784
	133067592194880 [label=NativeBatchNormBackward0]
	133067592195024 -> 133067592194880
	133067592195024 [label=ConvolutionBackward0]
	133067592195216 -> 133067592195024
	133067592195216 [label=ReluBackward0]
	133067592195360 -> 133067592195216
	133067592195360 [label=NativeBatchNormBackward0]
	133067592195456 -> 133067592195360
	133067592195456 [label=ConvolutionBackward0]
	133067592195648 -> 133067592195456
	133067592195648 [label=ReluBackward0]
	133067592195792 -> 133067592195648
	133067592195792 [label=AddBackward0]
	133067592195888 -> 133067592195792
	133067592195888 [label=NativeBatchNormBackward0]
	133067592196032 -> 133067592195888
	133067592196032 [label=ConvolutionBackward0]
	133067592196224 -> 133067592196032
	133067592196224 [label=ReluBackward0]
	133067592196368 -> 133067592196224
	133067592196368 [label=NativeBatchNormBackward0]
	133067592196464 -> 133067592196368
	133067592196464 [label=ConvolutionBackward0]
	133067592195840 -> 133067592196464
	133067592195840 [label=ReluBackward0]
	133067592196752 -> 133067592195840
	133067592196752 [label=AddBackward0]
	133067592196848 -> 133067592196752
	133067592196848 [label=NativeBatchNormBackward0]
	133067592196992 -> 133067592196848
	133067592196992 [label=ConvolutionBackward0]
	133067592197184 -> 133067592196992
	133067592197184 [label=ReluBackward0]
	133067592197328 -> 133067592197184
	133067592197328 [label=NativeBatchNormBackward0]
	133067592197424 -> 133067592197328
	133067592197424 [label=ConvolutionBackward0]
	133067592197616 -> 133067592197424
	133067592197616 [label=ReluBackward0]
	133067592197760 -> 133067592197616
	133067592197760 [label=AddBackward0]
	133067592197856 -> 133067592197760
	133067592197856 [label=NativeBatchNormBackward0]
	133067592198000 -> 133067592197856
	133067592198000 [label=ConvolutionBackward0]
	133067592198192 -> 133067592198000
	133067592198192 [label=ReluBackward0]
	133067592198336 -> 133067592198192
	133067592198336 [label=NativeBatchNormBackward0]
	133067592198432 -> 133067592198336
	133067592198432 [label=ConvolutionBackward0]
	133067592197808 -> 133067592198432
	133067592197808 [label=ReluBackward0]
	133067592198720 -> 133067592197808
	133067592198720 [label=AddBackward0]
	133067592198816 -> 133067592198720
	133067592198816 [label=NativeBatchNormBackward0]
	133067592198960 -> 133067592198816
	133067592198960 [label=ConvolutionBackward0]
	133067592199152 -> 133067592198960
	133067592199152 [label=ReluBackward0]
	133067592199296 -> 133067592199152
	133067592199296 [label=NativeBatchNormBackward0]
	133067592199392 -> 133067592199296
	133067592199392 [label=ConvolutionBackward0]
	133067592199584 -> 133067592199392
	133067592199584 [label=ReluBackward0]
	133067592199728 -> 133067592199584
	133067592199728 [label=AddBackward0]
	133067592199824 -> 133067592199728
	133067592199824 [label=NativeBatchNormBackward0]
	133067592199968 -> 133067592199824
	133067592199968 [label=ConvolutionBackward0]
	133067592200160 -> 133067592199968
	133067592200160 [label=ReluBackward0]
	133067592200304 -> 133067592200160
	133067592200304 [label=NativeBatchNormBackward0]
	133067592200400 -> 133067592200304
	133067592200400 [label=ConvolutionBackward0]
	133067592199776 -> 133067592200400
	133067592199776 [label=ReluBackward0]
	133067592200688 -> 133067592199776
	133067592200688 [label=AddBackward0]
	133067592200784 -> 133067592200688
	133067592200784 [label=NativeBatchNormBackward0]
	133067592200928 -> 133067592200784
	133067592200928 [label=ConvolutionBackward0]
	133067592201120 -> 133067592200928
	133067592201120 [label=ReluBackward0]
	133067592201264 -> 133067592201120
	133067592201264 [label=NativeBatchNormBackward0]
	133067592201360 -> 133067592201264
	133067592201360 [label=ConvolutionBackward0]
	133067592200736 -> 133067592201360
	133067592200736 [label=MaxPool2DWithIndicesBackward0]
	133067591913424 -> 133067592200736
	133067591913424 [label=ReluBackward0]
	133067592201696 -> 133067591913424
	133067592201696 [label=NativeBatchNormBackward0]
	133067592201792 -> 133067592201696
	133067592201792 [label=ConvolutionBackward0]
	133067592201984 -> 133067592201792
	133067686991968 [label="model.0.m.0.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	133067686991968 -> 133067592201984
	133067592201984 [label=AccumulateGrad]
	133067592201744 -> 133067592201696
	133067686992288 [label="model.0.m.1.weight
 (64)" fillcolor=lightblue]
	133067686992288 -> 133067592201744
	133067592201744 [label=AccumulateGrad]
	133067592201600 -> 133067592201696
	133067661046592 [label="model.0.m.1.bias
 (64)" fillcolor=lightblue]
	133067661046592 -> 133067592201600
	133067592201600 [label=AccumulateGrad]
	133067592201552 -> 133067592201360
	133067591563888 [label="model.0.m.4.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	133067591563888 -> 133067592201552
	133067592201552 [label=AccumulateGrad]
	133067592201312 -> 133067592201264
	133067591564208 [label="model.0.m.4.0.bn1.weight
 (64)" fillcolor=lightblue]
	133067591564208 -> 133067592201312
	133067592201312 [label=AccumulateGrad]
	133067592201168 -> 133067592201264
	133067591564928 [label="model.0.m.4.0.bn1.bias
 (64)" fillcolor=lightblue]
	133067591564928 -> 133067592201168
	133067592201168 [label=AccumulateGrad]
	133067592201072 -> 133067592200928
	133067591561168 [label="model.0.m.4.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	133067591561168 -> 133067592201072
	133067592201072 [label=AccumulateGrad]
	133067592200880 -> 133067592200784
	133067591561568 [label="model.0.m.4.0.bn2.weight
 (64)" fillcolor=lightblue]
	133067591561568 -> 133067592200880
	133067592200880 [label=AccumulateGrad]
	133067592200832 -> 133067592200784
	133067591562448 [label="model.0.m.4.0.bn2.bias
 (64)" fillcolor=lightblue]
	133067591562448 -> 133067592200832
	133067592200832 [label=AccumulateGrad]
	133067592200736 -> 133067592200688
	133067592200592 -> 133067592200400
	133067591560768 [label="model.0.m.4.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	133067591560768 -> 133067592200592
	133067592200592 [label=AccumulateGrad]
	133067592200352 -> 133067592200304
	133067591559968 [label="model.0.m.4.1.bn1.weight
 (64)" fillcolor=lightblue]
	133067591559968 -> 133067592200352
	133067592200352 [label=AccumulateGrad]
	133067592200208 -> 133067592200304
	133067591560048 [label="model.0.m.4.1.bn1.bias
 (64)" fillcolor=lightblue]
	133067591560048 -> 133067592200208
	133067592200208 [label=AccumulateGrad]
	133067592200112 -> 133067592199968
	133067591558288 [label="model.0.m.4.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	133067591558288 -> 133067592200112
	133067592200112 [label=AccumulateGrad]
	133067592199920 -> 133067592199824
	133067591556368 [label="model.0.m.4.1.bn2.weight
 (64)" fillcolor=lightblue]
	133067591556368 -> 133067592199920
	133067592199920 [label=AccumulateGrad]
	133067592199872 -> 133067592199824
	133067591558368 [label="model.0.m.4.1.bn2.bias
 (64)" fillcolor=lightblue]
	133067591558368 -> 133067592199872
	133067592199872 [label=AccumulateGrad]
	133067592199776 -> 133067592199728
	133067592199536 -> 133067592199392
	133067591554688 [label="model.0.m.5.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	133067591554688 -> 133067592199536
	133067592199536 [label=AccumulateGrad]
	133067592199344 -> 133067592199296
	133067591554288 [label="model.0.m.5.0.bn1.weight
 (128)" fillcolor=lightblue]
	133067591554288 -> 133067592199344
	133067592199344 [label=AccumulateGrad]
	133067592199200 -> 133067592199296
	133067591553888 [label="model.0.m.5.0.bn1.bias
 (128)" fillcolor=lightblue]
	133067591553888 -> 133067592199200
	133067592199200 [label=AccumulateGrad]
	133067592199104 -> 133067592198960
	133067591552528 [label="model.0.m.5.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	133067591552528 -> 133067592199104
	133067592199104 [label=AccumulateGrad]
	133067592198912 -> 133067592198816
	133067591552608 [label="model.0.m.5.0.bn2.weight
 (128)" fillcolor=lightblue]
	133067591552608 -> 133067592198912
	133067592198912 [label=AccumulateGrad]
	133067592198864 -> 133067592198816
	133067591552928 [label="model.0.m.5.0.bn2.bias
 (128)" fillcolor=lightblue]
	133067591552928 -> 133067592198864
	133067592198864 [label=AccumulateGrad]
	133067592198768 -> 133067592198720
	133067592198768 [label=NativeBatchNormBackward0]
	133067592199680 -> 133067592198768
	133067592199680 [label=ConvolutionBackward0]
	133067592199584 -> 133067592199680
	133067592199488 -> 133067592199680
	133067591555808 [label="model.0.m.5.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	133067591555808 -> 133067592199488
	133067592199488 [label=AccumulateGrad]
	133067592199248 -> 133067592198768
	133067591556688 [label="model.0.m.5.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	133067591556688 -> 133067592199248
	133067592199248 [label=AccumulateGrad]
	133067592199008 -> 133067592198768
	133067591556288 [label="model.0.m.5.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	133067591556288 -> 133067592199008
	133067592199008 [label=AccumulateGrad]
	133067592198624 -> 133067592198432
	133067591551328 [label="model.0.m.5.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	133067591551328 -> 133067592198624
	133067592198624 [label=AccumulateGrad]
	133067592198384 -> 133067592198336
	133067591550928 [label="model.0.m.5.1.bn1.weight
 (128)" fillcolor=lightblue]
	133067591550928 -> 133067592198384
	133067592198384 [label=AccumulateGrad]
	133067592198240 -> 133067592198336
	133067591552128 [label="model.0.m.5.1.bn1.bias
 (128)" fillcolor=lightblue]
	133067591552128 -> 133067592198240
	133067592198240 [label=AccumulateGrad]
	133067592198144 -> 133067592198000
	133067591549328 [label="model.0.m.5.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	133067591549328 -> 133067592198144
	133067592198144 [label=AccumulateGrad]
	133067592197952 -> 133067592197856
	133067591549248 [label="model.0.m.5.1.bn2.weight
 (128)" fillcolor=lightblue]
	133067591549248 -> 133067592197952
	133067592197952 [label=AccumulateGrad]
	133067592197904 -> 133067592197856
	133067591565248 [label="model.0.m.5.1.bn2.bias
 (128)" fillcolor=lightblue]
	133067591565248 -> 133067592197904
	133067592197904 [label=AccumulateGrad]
	133067592197808 -> 133067592197760
	133067592197568 -> 133067592197424
	133067659278240 [label="model.0.m.6.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	133067659278240 -> 133067592197568
	133067592197568 [label=AccumulateGrad]
	133067592197376 -> 133067592197328
	133067659278320 [label="model.0.m.6.0.bn1.weight
 (256)" fillcolor=lightblue]
	133067659278320 -> 133067592197376
	133067592197376 [label=AccumulateGrad]
	133067592197232 -> 133067592197328
	133067659278640 [label="model.0.m.6.0.bn1.bias
 (256)" fillcolor=lightblue]
	133067659278640 -> 133067592197232
	133067592197232 [label=AccumulateGrad]
	133067592197136 -> 133067592196992
	133067659276320 [label="model.0.m.6.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	133067659276320 -> 133067592197136
	133067592197136 [label=AccumulateGrad]
	133067592196944 -> 133067592196848
	133067659277120 [label="model.0.m.6.0.bn2.weight
 (256)" fillcolor=lightblue]
	133067659277120 -> 133067592196944
	133067592196944 [label=AccumulateGrad]
	133067592196896 -> 133067592196848
	133067659276240 [label="model.0.m.6.0.bn2.bias
 (256)" fillcolor=lightblue]
	133067659276240 -> 133067592196896
	133067592196896 [label=AccumulateGrad]
	133067592196800 -> 133067592196752
	133067592196800 [label=NativeBatchNormBackward0]
	133067592197712 -> 133067592196800
	133067592197712 [label=ConvolutionBackward0]
	133067592197616 -> 133067592197712
	133067592197520 -> 133067592197712
	133067659276080 [label="model.0.m.6.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	133067659276080 -> 133067592197520
	133067592197520 [label=AccumulateGrad]
	133067592197280 -> 133067592196800
	133067659279920 [label="model.0.m.6.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	133067659279920 -> 133067592197280
	133067592197280 [label=AccumulateGrad]
	133067592197040 -> 133067592196800
	133067659279840 [label="model.0.m.6.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	133067659279840 -> 133067592197040
	133067592197040 [label=AccumulateGrad]
	133067592196656 -> 133067592196464
	133067659274400 [label="model.0.m.6.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	133067659274400 -> 133067592196656
	133067592196656 [label=AccumulateGrad]
	133067592196416 -> 133067592196368
	133067659274480 [label="model.0.m.6.1.bn1.weight
 (256)" fillcolor=lightblue]
	133067659274480 -> 133067592196416
	133067592196416 [label=AccumulateGrad]
	133067592196272 -> 133067592196368
	133067659274880 [label="model.0.m.6.1.bn1.bias
 (256)" fillcolor=lightblue]
	133067659274880 -> 133067592196272
	133067592196272 [label=AccumulateGrad]
	133067592196176 -> 133067592196032
	133067659272640 [label="model.0.m.6.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	133067659272640 -> 133067592196176
	133067592196176 [label=AccumulateGrad]
	133067592195984 -> 133067592195888
	133067659273600 [label="model.0.m.6.1.bn2.weight
 (256)" fillcolor=lightblue]
	133067659273600 -> 133067592195984
	133067592195984 [label=AccumulateGrad]
	133067592195936 -> 133067592195888
	133067659272320 [label="model.0.m.6.1.bn2.bias
 (256)" fillcolor=lightblue]
	133067659272320 -> 133067592195936
	133067592195936 [label=AccumulateGrad]
	133067592195840 -> 133067592195792
	133067592195600 -> 133067592195456
	133067659268160 [label="model.0.m.7.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	133067659268160 -> 133067592195600
	133067592195600 [label=AccumulateGrad]
	133067592195408 -> 133067592195360
	133067659265600 [label="model.0.m.7.0.bn1.weight
 (512)" fillcolor=lightblue]
	133067659265600 -> 133067592195408
	133067592195408 [label=AccumulateGrad]
	133067592195264 -> 133067592195360
	133067659266320 [label="model.0.m.7.0.bn1.bias
 (512)" fillcolor=lightblue]
	133067659266320 -> 133067592195264
	133067592195264 [label=AccumulateGrad]
	133067592195168 -> 133067592195024
	133067659270880 [label="model.0.m.7.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	133067659270880 -> 133067592195168
	133067592195168 [label=AccumulateGrad]
	133067592194976 -> 133067592194880
	133067659270320 [label="model.0.m.7.0.bn2.weight
 (512)" fillcolor=lightblue]
	133067659270320 -> 133067592194976
	133067592194976 [label=AccumulateGrad]
	133067592194928 -> 133067592194880
	133067658752592 [label="model.0.m.7.0.bn2.bias
 (512)" fillcolor=lightblue]
	133067658752592 -> 133067592194928
	133067592194928 [label=AccumulateGrad]
	133067592194832 -> 133067592194784
	133067592194832 [label=NativeBatchNormBackward0]
	133067592195744 -> 133067592194832
	133067592195744 [label=ConvolutionBackward0]
	133067592195648 -> 133067592195744
	133067592195552 -> 133067592195744
	133067659271040 [label="model.0.m.7.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	133067659271040 -> 133067592195552
	133067592195552 [label=AccumulateGrad]
	133067592195312 -> 133067592194832
	133067659271520 [label="model.0.m.7.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	133067659271520 -> 133067592195312
	133067592195312 [label=AccumulateGrad]
	133067592195072 -> 133067592194832
	133067659270800 [label="model.0.m.7.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	133067659270800 -> 133067592195072
	133067592195072 [label=AccumulateGrad]
	133067592194688 -> 133067592194496
	133067959048128 [label="model.0.m.7.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	133067959048128 -> 133067592194688
	133067592194688 [label=AccumulateGrad]
	133067592194448 -> 133067592194400
	133067959049648 [label="model.0.m.7.1.bn1.weight
 (512)" fillcolor=lightblue]
	133067959049648 -> 133067592194448
	133067592194448 [label=AccumulateGrad]
	133067592194304 -> 133067592194400
	133067959048608 [label="model.0.m.7.1.bn1.bias
 (512)" fillcolor=lightblue]
	133067959048608 -> 133067592194304
	133067592194304 [label=AccumulateGrad]
	133067592194208 -> 133067592194064
	133067959050928 [label="model.0.m.7.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	133067959050928 -> 133067592194208
	133067592194208 [label=AccumulateGrad]
	133067592194016 -> 133067591910928
	133067959049248 [label="model.0.m.7.1.bn2.weight
 (512)" fillcolor=lightblue]
	133067959049248 -> 133067592194016
	133067592194016 [label=AccumulateGrad]
	133067592193968 -> 133067591910928
	133067959049328 [label="model.0.m.7.1.bn2.bias
 (512)" fillcolor=lightblue]
	133067959049328 -> 133067592193968
	133067592193968 [label=AccumulateGrad]
	133067592193872 -> 133067659235856
	133067592193824 -> 133067592193680
	133067659271920 [label="model.1.conv.conv.weight
 (1280, 512, 1, 1)" fillcolor=lightblue]
	133067659271920 -> 133067592193824
	133067592193824 [label=AccumulateGrad]
	133067592193776 -> 133067591681648
	133067659277440 [label="model.1.conv.bn.weight
 (1280)" fillcolor=lightblue]
	133067659277440 -> 133067592193776
	133067592193776 [label=AccumulateGrad]
	133067592193728 -> 133067591681648
	133067659268400 [label="model.1.conv.bn.bias
 (1280)" fillcolor=lightblue]
	133067659268400 -> 133067592193728
	133067592193728 [label=AccumulateGrad]
	133067659238400 -> 133067658894816
	133067659238400 [label=TBackward0]
	133067591680064 -> 133067659238400
	133067591554208 [label="model.1.linear.weight
 (1000, 1280)" fillcolor=lightblue]
	133067591554208 -> 133067591680064
	133067591680064 [label=AccumulateGrad]
	133067658894816 -> 133067591928880
}
